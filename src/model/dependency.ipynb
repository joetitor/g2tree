{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import networkx.algorithms as nxalg\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from data_utils import SymbolsManager\n",
    "from sys import path\n",
    "from data_utils import convert_to_tree\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic configuration\n",
    "\n",
    "data_dir = \"../dataset/\"\n",
    "batch_size = 20\n",
    "min_freq = 2\n",
    "max_vocab_size = 15000\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111860b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the corenlp server first\n",
    "class InputPreprocessor(object):\n",
    "    def __init__(self, url = 'http://localhost:9000'):\n",
    "        self.nlp = StanfordCoreNLP(url)\n",
    "\n",
    "    def featureExtract(self,src_text,whiteSpace=True):\n",
    "        data = {}\n",
    "        output = self.nlp.annotate(src_text.strip(), properties={\n",
    "        'annotators': \"tokenize,ssplit,pos,depparse\",\n",
    "        \"tokenize.options\":\"splitHyphenated=true,normalizeParentheses=false\",\n",
    "\t\t\"tokenize.whitespace\": whiteSpace,\n",
    "        'ssplit.isOneSentence': True,\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "       \n",
    "        # core_arguments = {\"nsubj\", \"xcomp\", \"iobj\"}\n",
    "        core_arguments = {u'cc': 95, u'ccomp': 47, u'conj': 88, u'neg': 42, u'dobj': 317, \n",
    "         u'nsubj': 370, u'compound': 630, u'case': 558, u'cop': 42, u'dep': 107, \n",
    "         u'det': 496, u'nmod': 523, u'amod': 90, u'acl:relcl': 72, u'iobj': 81, u'expl': 138}\n",
    "        \n",
    "        snt = output['sentences'][0][\"tokens\"]\n",
    "        depency = output['sentences'][0][\"basicDependencies\"]\n",
    "        data[\"word_list\"] = []\n",
    "        data[\"fw_adj\"] = {}\n",
    "        for snt_tok in snt:\n",
    "            data[\"word_list\"].append(str(snt_tok['word']))\n",
    "        seq_len = len(data[\"word_list\"])\n",
    "        data[\"seq_len\"] = seq_len\n",
    "\n",
    "        for idx in range(seq_len):\n",
    "            data[\"fw_adj\"][idx] = []\n",
    "        dep_index = 0\n",
    "        for deps in depency:\n",
    "            if deps['dep'] in core_arguments:\n",
    "                # if deps['dep'] != \"neg\" or data['word_list'][deps['governor'] - 1] == \"outsid\":\n",
    "                if deps['dep'] != \"ref_value\":\n",
    "                    if str(deps['dep']) not in data[\"word_list\"]:\n",
    "                        data[\"word_list\"].append(str(deps['dep']))\n",
    "                    # data[\"fw_adj\"][deps['governor'] - 1].append([deps['dependent'] - 1, seq_len + dep_index])\n",
    "                    data[\"fw_adj\"][deps['governor'] - 1].append((deps['dependent'] - 1, str(deps['dep'])))\n",
    "            # else:\n",
    "            #     if data['word_list'][deps['dependent'] - 1] == \"outsid\":\n",
    "            #         # print \"ASDASD\"\n",
    "            #         data[\"word_list\"].append(\"neg\")\n",
    "            #         data[\"fw_adj\"][deps['governor'] - 1].append((deps['dependent'] - 1, \"neg\"))\n",
    "                dep_index += 1\n",
    "        return data\n",
    "\n",
    "def read_parsed_result(input_file):\n",
    "        graphs_new = []\n",
    "        with open(input_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                jo = json.loads(line)\n",
    "                graphs_new.append(jo)\n",
    "        return graphs_new\n",
    "\n",
    "def begin_parsing_dep():\n",
    "    feature_extractor = InputPreprocessor()\n",
    "\n",
    "    src_txt = []\n",
    "    with open(\"{}/{}.txt\".format(data_dir, \"train\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            src_txt.append(line.strip().split('\\t')[0])\n",
    "\n",
    "    with open(\"{}/{}\".format(data_dir, \"dependency_parsed_result.train\"), \"w\") as f:\n",
    "        for sentence in tqdm.tqdm(src_txt):\n",
    "            parsed_result = feature_extractor.featureExtract(sentence)\n",
    "            f.write(json.dumps(parsed_result) + '\\n')\n",
    "\n",
    "    src_txt = []\n",
    "    with open(\"{}/{}.txt\".format(data_dir, \"test\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            src_txt.append(line.strip().split('\\t')[0])\n",
    "\n",
    "    with open(\"{}/{}\".format(data_dir, \"dependency_parsed_result.test\"), \"w\") as f:\n",
    "        for sentence in tqdm.tqdm(src_txt):\n",
    "            parsed_result = feature_extractor.featureExtract(sentence)\n",
    "            f.write(json.dumps(parsed_result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_with_wo_dep_info(output_file, src, graph_scale):\n",
    "    graph_list = []\n",
    "    batch_size = len(src)\n",
    "    for num in range(batch_size):\n",
    "        info = {}\n",
    "        graph = nx.DiGraph()\n",
    "        graph_node_size = src[num]['seq_len']\n",
    "        source_text = src[num]['word_list']\n",
    "        dependency_edge = src[num]['fw_adj']\n",
    "        for idx in range(graph_scale):\n",
    "            graph.add_node(idx)\n",
    "            if(idx >= 1 and idx <= graph_node_size - 1):\n",
    "                graph.add_edge(idx, idx-1)\n",
    "                graph.add_edge(idx-1, idx)\n",
    "            if(idx <= graph_node_size - 1) and (str(idx) in dependency_edge.keys()):\n",
    "                dependency_edge_list = dependency_edge[str(idx)]\n",
    "                for tmp in dependency_edge_list:\n",
    "                    dep_index = tmp[1]\n",
    "                    dep_index = source_text.index(tmp[1])\n",
    "                    graph.add_edge(idx, dep_index)\n",
    "                    graph.add_edge(dep_index, tmp[0])\n",
    "\n",
    "        adj_list = [sorted(n_dict.keys()) for nodes, n_dict in graph.adjacency()]\n",
    "\n",
    "        g_ids = {}\n",
    "        g_ids_features = {}\n",
    "        g_adj = {}\n",
    "        for i in range(graph_scale):\n",
    "            g_ids[i] = i\n",
    "\n",
    "            if i < len(source_text):\n",
    "                g_ids_features[i] = source_text[i]\n",
    "            else:\n",
    "                g_ids_features[i] = '<P>'\n",
    "            \n",
    "            g_adj[i] = adj_list[i]\n",
    "\n",
    "        info['g_ids'] = g_ids\n",
    "        info['g_ids_features'] = g_ids_features\n",
    "        info['g_adj'] = g_adj\n",
    "        info['word_list'] = source_text\n",
    "        graph_list.append(info)\n",
    "        \n",
    "    with open(output_file, \"a+\") as f:\n",
    "        for idx in range(len(graph_list)):\n",
    "            f.write(json.dumps(graph_list[idx]) + '\\n')\n",
    "\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process dep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_data_preprocess():\n",
    "    time_start = time.time()\n",
    "    word_manager = SymbolsManager(True)\n",
    "    word_manager.init_from_file(\"{}/vocab.q.txt\".format(data_dir), min_freq, max_vocab_size)\n",
    "    form_manager = SymbolsManager(True)\n",
    "    form_manager.init_from_file(\"{}/vocab.f.txt\".format(data_dir), 0, max_vocab_size)\n",
    "    print(word_manager.vocab_size)\n",
    "    print(form_manager.vocab_size)\n",
    "    data = []\n",
    "    with open(\"{}/{}.txt\".format(data_dir, \"train\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            l_list = line.split(\"\\t\")\n",
    "            w_list = l_list[0].strip().split(' ')\n",
    "            r_list = form_manager.get_symbol_idx_for_list(l_list[1].strip().split(' '))\n",
    "            cur_tree = convert_to_tree(r_list, 0, len(r_list), form_manager)\n",
    "\n",
    "            data.append((w_list, r_list, cur_tree))\n",
    "\n",
    "    out_graphfile = \"{}/graph.train\".format(data_dir)\n",
    "    if os.path.exists(out_graphfile):\n",
    "        os.remove(out_graphfile)\n",
    "    # generate batch graph here\n",
    "    if len(data) % batch_size != 0:\n",
    "        n = len(data)\n",
    "        for i in range(len(data)%batch_size):\n",
    "            data.insert(n-i-1, data[n-i-1])\n",
    "            \n",
    "    dependency_parsed_result = read_parsed_result(\"../dataset/dependency_parsed_result.train\")\n",
    "    if len(dependency_parsed_result) % batch_size != 0:\n",
    "        n = len(dependency_parsed_result)\n",
    "        for i in range(len(dependency_parsed_result)%batch_size):\n",
    "            dependency_parsed_result.insert(n-i-1, dependency_parsed_result[n-i-1])\n",
    "    print len(data)\n",
    "    print len(dependency_parsed_result)\n",
    "    index = 0\n",
    "    while index + batch_size <= len(data):\n",
    "        # generate graphs with order and dependency information\n",
    "        dependency_batch = [dependency_parsed_result[index+idx] for idx in range(batch_size)]\n",
    "        max_dependency_node_size = max([len(dependency_batch[idx]['word_list']) for idx in range(batch_size)])\n",
    "        dependency_graph_batch = create_with_wo_dep_info(out_graphfile, dependency_batch, max_dependency_node_size)\n",
    "        \n",
    "        for idx in range(len(dependency_batch)):\n",
    "            w_list_len = dependency_batch[idx]['seq_len']\n",
    "            w_list = dependency_batch[idx]['word_list'][w_list_len:]\n",
    "            for j in w_list:\n",
    "                if j not in word_manager.symbol2idx:\n",
    "                        word_manager.add_symbol(j)\n",
    "                        print \"{} Added.\".format(j)\n",
    "        index += batch_size\n",
    "\n",
    "    out_datafile = \"{}/train.pkl\".format(data_dir)\n",
    "    with open(out_datafile, \"wb\") as out_data:\n",
    "        pkl.dump(data, out_data)\n",
    "    \n",
    "    out_mapfile = \"{}/map.pkl\".format(data_dir)\n",
    "    with open(out_mapfile, \"wb\") as out_map:\n",
    "        pkl.dump([word_manager, form_manager], out_map)\n",
    "\n",
    "    print(word_manager.vocab_size)\n",
    "    print(form_manager.vocab_size)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print \"time used:\" + str(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def test_data_preprocess():\n",
    "    data = []\n",
    "    managers = pkl.load( open(\"{}/map.pkl\".format(data_dir), \"rb\" ) )\n",
    "    word_manager, form_manager = managers\n",
    "    with open(\"{}/{}.txt\".format(data_dir, \"test\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            l_list = line.split(\"\\t\")\n",
    "            w_list = l_list[0].strip().split(' ')\n",
    "            r_list = form_manager.get_symbol_idx_for_list(l_list[1].strip().split(' '))\n",
    "            cur_tree = convert_to_tree(r_list, 0, len(r_list), form_manager)\n",
    "            data.append((w_list, r_list, cur_tree))\n",
    "    out_datafile = \"{}/test.pkl\".format(data_dir)\n",
    "    with open(out_datafile, \"wb\") as out_data:\n",
    "        pkl.dump(data, out_data)\n",
    "\n",
    "    out_graphfile = \"{}/graph.test\".format(data_dir)\n",
    "    if os.path.exists(out_graphfile):\n",
    "        os.remove(out_graphfile)\n",
    "\n",
    "    # generate batch graph here\n",
    "    if len(data) % batch_size != 0:\n",
    "        n = len(data)\n",
    "        for i in range(len(data)%batch_size):\n",
    "            data.insert(n-i-1, data[n-i-1])\n",
    "    dependency_parsed_result = read_parsed_result(\"../dataset/dependency_parsed_result.test\")\n",
    "    if len(dependency_parsed_result) % batch_size != 0:\n",
    "        n = len(dependency_parsed_result)\n",
    "        for i in range(len(dependency_parsed_result)%batch_size):\n",
    "            dependency_parsed_result.insert(n-i-1, dependency_parsed_result[n-i-1])\n",
    "            \n",
    "    index = 0\n",
    "    while index + batch_size <= len(data):\n",
    "        # generate graphs with order and dependency information\n",
    "        dependency_batch = [dependency_parsed_result[index+idx] for idx in range(batch_size)]\n",
    "        max_dependency_node_size = max([len(dependency_batch[idx]['word_list']) for idx in range(batch_size)])\n",
    "        dependency_graph_batch = create_with_wo_dep_info(out_graphfile, dependency_batch, max_dependency_node_size)\n",
    "        \n",
    "        index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 83.65it/s]\n",
      "100%|██████████| 140/140 [00:01<00:00, 84.95it/s]\n"
     ]
    }
   ],
   "source": [
    "begin_parsing_dep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary file: ../dataset//vocab.q.txt\n",
      "loading vocabulary file: ../dataset//vocab.f.txt\n",
      "129\n",
      "52\n",
      "500\n",
      "500\n",
      "det Added.\n",
      "case Added.\n",
      "nmod Added.\n",
      "nsubj Added.\n",
      "ccomp Added.\n",
      "compound Added.\n",
      "dobj Added.\n",
      "expl Added.\n",
      "iobj Added.\n",
      "amod Added.\n",
      "dep Added.\n",
      "cop Added.\n",
      "neg Added.\n",
      "acl:relcl Added.\n",
      "cc Added.\n",
      "conj Added.\n",
      "145\n",
      "52\n",
      "time used:0.27027797699\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
